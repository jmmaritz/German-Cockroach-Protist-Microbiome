
## 18S rRNA gene amplicons - EUKARYOTE WORKFLOW

## QIIME data analysis workflow used in manuscript:
## Overlapping Community Composition of Gut and Fecal Microbiomes in Lab-Reared and Field-Collected German Cockroaches
## KAKUMANU ML, MARITZ JM, CARLTON JM, SCHAL C.

## All data analysis carried out on New York University's High Performance Compute cluster
## Parameters and file paths may vary on your system
## Edit commands as necessary to replicate these data analysis locally



############### Demultiplexing Raw Data - 18S rRNA ##############

## The adapter file includes both V9 region PCR primer sequences (1391F and EUKBR) and their reverse compliments for 'simple' clipping
## As well as the 1391F sequence and r/c of EUKBR for 'palindrome' clipping

#!/bin/sh

#SBATCH --nodes=1
#SBATCH --cpus-per-task=8
#SBATCH --time=5:00:00
#SBATCH --mem=25GB
#SBATCH --job-name=trim_join_dmex_run1
#SBATCH --output=slurm_trim_join_dmex_run1_%j.out
	
module purge
module load trimmomatic/0.36
module load seqtk/intel/1.2-r94
module load qiime/intel/1.9.1
module load ea-utils/intel/1.1.2

# change these paths according to where the project files are locally
PRIMERS=/Primers
RUNDIR=/Cockroach/Run1
cd $RUNDIR

# Remove adapters

srun java -jar /share/apps/trimmomatic/0.36/trimmomatic-0.36.jar PE -threads 6 -trimlog Run1_trimmomatic_log.txt \
	$RUNDIR/000000000-AKKFY_l01n01.3310000004aed7.fastq.gz \
	$RUNDIR/000000000-AKKFY_l01n03.3330000004aed1.fastq.gz \
	R1_adapters_trimmed_pe.fastq.gz R1_adapters_trimmed_se.fastq.gz R2_adapters_trimmed_pe.fastq.gz R2_adapters_trimmed_se.fastq.gz \
	ILLUMINACLIP:$PRIMERS/V9_adapters_palindrome.fa:2:30:10:3:true MINLEN:0

	# 3 is the <minAdapterLength> option
	# TRUE passes the <keepBothReads> flag to keep all reads
	# MINLEN:0 makes sure all reads are kept

srun gunzip R1_adapters_trimmed_pe.fastq.gz

# get the ids of all the paired-end reads that are kept and format them for seqtk
	# seqtk does not like the '@' at the beginning of the read names or the '1:N:0:' after the space at the end
grep "@M02455" R1_adapters_trimmed_pe.fastq | perl -pe 's/^@//g' | perl -pe 's/ 1:N:0://g' > Run1_adapters_trimmed_remaining_pe_read_IDs.lst

# Make sure that the barcode file exactly matches that of the reads, otherwise you will get a QIIME error
srun seqtk subseq $RUNDIR/000000000-AKKFY_l01n02.3320000004aed4.fastq.gz $RUNDIR/Run1_adapters_trimmed_remaining_pe_read_IDs.lst > $RUNDIR/Run1_adapters_trimmed_pe_barcodes.fastq.gz

# Join the trimmed, paired reads
srun join_paired_ends.py -f $RUNDIR/R1_adapters_trimmed_pe.fastq \
    -r $RUNDIR/R2_adapters_trimmed_pe.fastq.gz \
    -b $RUNDIR/Run1_adapters_trimmed_pe_barcodes.fastq.gz \
    -o $RUNDIR/fastq-join_joined_trimmed_min5 \
    -j 5 -p 20

# Demultiplex
srun split_libraries_fastq.py -i $RUNDIR/fastq-join_joined_trimmed_min5/fastqjoin.join.fastq \
    -b $RUNDIR/fastq-join_joined_trimmed_min5/fastqjoin.join_barcodes.fastq \
    -m $RUNDIR/CR_18S_run1_mapp_QIIME.txt \
    -o $RUNDIR \
    --rev_comp_mapping_barcodes -q 19 -r 5 -p 0.70

mv seqs.fna Run1_seqs_trimmed_joined_dmex.fna


## SECOND RUN

#!/bin/sh

#SBATCH --nodes=1
#SBATCH --cpus-per-task=8
#SBATCH --time=5:00:00
#SBATCH --mem=25GB
#SBATCH --job-name=trim_join_dmex_run2
#SBATCH --output=slurm_trim_join_dmex_run2_%j.out
	
module purge
module load trimmomatic/0.36
module load seqtk/intel/1.2-r94
module load qiime/intel/1.9.1
module load ea-utils/intel/1.1.2

# change these paths according to where the project files are locally
PRIMERS=/Primers
RUNDIR=/Cockroach/Run2
cd $RUNDIR

java -jar /share/apps/trimmomatic/0.36/trimmomatic-0.36.jar PE -threads 6 -trimlog Run2_trimmomatic_log.txt \
	$RUNDIR/000000000-ANE69_l01n01.331000000663cf.fastq.gz \
	$RUNDIR/000000000-ANE69_l01n03.333000000663c9.fastq.gz \
	R1_adapters_trimmed_pe2.fastq.gz R1_adapters_trimmed_se2.fastq.gz R2_adapters_trimmed_pe2.fastq.gz R2_adapters_trimmed_se2.fastq.gz \
	ILLUMINACLIP:$PRIMERS/V9_adapters_palindrome.fa:2:30:10:3:true MINLEN:0

gunzip R1_adapters_trimmed_pe2.fastq.gz

grep "@M02455" R1_adapters_trimmed_pe2.fastq | perl -pe 's/^@//g' | perl -pe 's/ 1:N:0://g' > Run2_adapters_trimmed_remaining_pe_read_IDs.lst

seqtk subseq $RUNDIR/000000000-ANE69_l01n02.332000000663cc.fastq.gz $RUNDIR/Run2_adapters_trimmed_remaining_pe_read_IDs.lst > $RUNDIR/Run2_adapters_trimmed_pe_barcodes.fastq.gz

join_paired_ends.py -f $RUNDIR/R1_adapters_trimmed_pe2.fastq \
    -r $RUNDIR/R2_adapters_trimmed_pe2.fastq.gz \
    -b $RUNDIR/Run2_adapters_trimmed_pe_barcodes.fastq.gz \
    -o $RUNDIR/fastq-join_joined_trimmed_min5 \
    -j 5 -p 20

srun split_libraries_fastq.py -i $RUNDIR/fastq-join_joined_trimmed_min5/fastqjoin.join.fastq \
    -b $RUNDIR/fastq-join_joined_trimmed_min5/fastqjoin.join_barcodes.fastq \
    -m $RUNDIR/CR_18S_run2_mapp_QIIME.txt \
    -o $RUNDIR \
    --rev_comp_mapping_barcodes -q 19 -r 5 -p 0.70

mv seqs.fna Run2_seqs_trimmed_joined_dmex.fna

# Concatenate all of the sequences together
cd ../
mkdir Seqs

cat Run1/Run1_seqs_trimmed_joined_dmex.fna Run2/Run2_seqs_trimmed_joined_dmex.fna > Seqs/CR_18S_seqs.fna



############### 98% DE NOVO OTU Picking - 18S rRNA ##############

#!/bin/sh

#SBATCH --nodes=1
#SBATCH --cpus-per-task=8
#SBATCH --time=60:00:00
#SBATCH --mem=160GB
#SBATCH --job-name=otus
#SBATCH --output=slurm_otus_%j.out
	
module purge
module load python/intel/2.7.12

# Change these paths according to where the project files are locally
PYTHON=/Python_scripts
SEQS=/Cockroach/Seqs
RUNDIR=/Cockroach/Uparse

cd $RUNDIR

# Dereplicate and remove singletons
/share/apps/qiime/1.9.1/usearch64/usearch8.0.1623_i86linux64 -derep_fulllength $SEQS/CR_18S_seqs.fna \
	-fastaout $RUNDIR/CR_18S_uniques_min2.fasta \
	-sizeout -minuniquesize 2

# Cluster OTUs
/share/apps/qiime/1.9.1/usearch64/usearch8.0.1623_i86linux64 --cluster_otus $RUNDIR/CR_18S_uniques_min2.fasta \
	-otus $RUNDIR/CR_18S_uniques_min2_otus_min2.fasta \
	-uparseout $RUNDIR/CR_18S_uniques_min2_otus_min2.up \
	-relabel OTU_ -otu_radius_pct 2 -minsize 2

	# -relable OTU_ changes the name of rep set sequences to OTU_
	# -otu_radius_pct 2 clusters at 98%
	# chimera filtering happens automatically

# Map the reads back to the OTUs
/share/apps/qiime/1.9.1/usearch64/usearch8.0.1623_i86linux64 -usearch_global $SEQS/CR_18S_seqs.fna \
	-db $RUNDIR/CR_18S_uniques_min2_otus_min2.fasta \
	-strand plus -id 0.98 \
	-maxaccepts 8 -maxrejects 64 \
	-top_hit_only \
	-uc $RUNDIR/CR_18S_otu_map.uc

	# a given read may match two or more OTUs at the given identity threshold. 
	# In such cases, usearch_global will tend to assign the read to the OTU with highest identity 
	# and break ties arbitrarily 
	# -maxaccepts 8 -maxrejects 64 help to mitigate issues above and get more consistent OTU assignments
	# -top_hit_only breaks ties consistently by choosing the first target sequence in database order
	
# Convert the UPARSE .uc file to a QIIME compatible OTU mat .txt file
	# python script modified from the readmap2qiime.py script provided by USEARCH
	# https://drive5.com/otupipe/readmap2qiime.txt

python $PYTHON/readmap2qiime.py $RUNDIR/CR_18S_otu_map.uc > $RUNDIR/CR_18S_otus.txt

# Create file for rep set filtering to only the OTUs with reads mapping to them
cut -f 10 $RUNDIR/CR_18S_otu_map.uc | sort -u > $RUNDIR/rep_seqs_to_keep.txt

	# You need to open .txt file and reemove the '*' before filtering the fasta file


############### Assign Taxonomy ##############

#!/bin/bash

#SBATCH --nodes=1
#SBATCH --cpus-per-task=4
#SBATCH --time=10:00:00
#SBATCH --mem=60GB
#SBATCH --job-name=tax
#SBATCH --output=slurm_tax_%j.out

module purge
module load blast/intel/2.2.22
module load qiime/intel/1.9.1 

# Change these paths according to where the project files are locally
RUNDIR=/Cockroach/Tax_assignments
TAX=/Silva_111_curated_V2_2016/Euk_only_curated
TAX99=/Silva_111_curated_V2_2016/99_clusters
UPARSE=/Cockroach/Uparse
SEQS=/Cockroach/Seqs

cd $RUNDIR

# Filter the rep set to only the OTUs with reads mapping to them
	# Not all the OTUs end up with mapped reads, not sure why, but it happens, generally its only a few
srun filter_fasta.py -f $UPARSE/CR_18S_uniques_min2_otus_min2.fasta -o $SEQS/CR_18S_rep_set.fasta -s $UPARSE/rep_seqs_to_keep.txt

# Assign taxonomy in two steps, first with  curated Silva database from:
	# An 18S rRNA Workflow for characterizing protists in sewage, with a focus on zoonotic Trichomonads
	# Maritz, J.M., Rogers, K.H., Rock, T.M. et al. Microb Ecol (2017) 74: 923.
	# Figshare [https://doi.org/10.6084/m9.figshare.3114850.v1].

srun parallel_assign_taxonomy_blast.py -i $RUNDIR/CR_18S_rep_set.fasta -m blast \
	-r $TAX/Euk_Silva_111_curated.fasta \
	-t $TAX/Euk_Silva_111_taxa_map_curated.txt \
	-o $RUNDIR -e 1e-20 -O 4

# Next use SILVA files clustered at 99% on anything that was not assigned
	# this is because if you don't you end up with a huge portion in the No BLAST hit category
	# they have a hit, they just aren't eukaryotes or are an uncultured protist
	# second step required so these results can be filtered out and not artifically inflate the unassigned category

# Get OTU ids with no blast hit
grep "No blast hit" $RUNDIR/CR_18S_rep_set_tax_assignments.txt | cut -f 1 > $RUNDIR/Unassigned_otu_ids.txt

# Get OTU ids that were assigned taxonomy
grep -v "No blast hit" $RUNDIR/CR_18S_rep_set_tax_assignments.txt > $RUNDIR/Assigned_otus.txt

# Filter the fasta file to get unassigned sequences
srun filter_fasta.py -f $RUNDIR/CR_18S_rep_set.fasta -s $RUNDIR/Unassigned_otu_ids.txt -o $RUNDIR/Unassigned_otus.fasta

# Then with SILVA 111 clustered at 99%
srun parallel_assign_taxonomy_blast.py -i $RUNDIR/Unassigned_otus.fasta -m blast \
	-r $TAX99/99_Silva_111_rep_set.fasta \
	-t $TAX99/99_Silva_111_taxa_map.txt \
	-o $RUNDIR -e 1e-20 -O 4

# Combine the taxonomy assignments
cat $RUNDIR/Assigned_otus.txt $RUNDIR/Unassigned_otus_tax_assignments.txt > $RUNDIR/CR_18S_otus_tax_assignments_edited.txt

# Merge everything into a fasta file
# Remove "OTU_" from the OTUs as its not compatible with QIIME
cut -f 1 CR_18S_otus_tax_assignments_edited.txt | perl -pe 's/;.*?;//g' | perl -pe 's/^OTU_//g' > CR_18S_otus_tax_assignments_ids.txt

cut -f 2-4 CR_18S_otus_tax_assignments_edited.txt > CR_18_otus_tax_assignments_results.txt

paste CR_18S_otus_tax_assignments_ids.txt CR_18_otus_tax_assignments_results.txt > CR_18S_otus_tax_assignments.txt


############### Make and Filter the OTU table ##############

#!/bin/bash

#SBATCH --nodes=1
#SBATCH --cpus-per-task=2
#SBATCH --time=01:00:00
#SBATCH --mem=10GB
#SBATCH --job-name=otu_filter
#SBATCH --output=slurm_otu_filter_%j.out

module purge
module load qiime/intel/1.9.1 

# Change these paths according to where the project files are locally
RUNDIR=/Cockroach/OTUs
UPARSE=/Cockroach/Uparse
TAX=/Cockroach/Tax_assignments

cd $RUNDIR

# Make the OTU table
srun make_otu_table.py -i $UPARSE/CR_18S_otus.txt -t $TAX/CR_18S_otus_tax_assignments.txt \
	-m $RUNDIR/CR_mapp.txt \
	-o $RUNDIR/CR_18S_otu_table_w_tax.biom

srun biom summarize-table -i $RUNDIR/CR_18S_otu_table_w_tax.biom \
	-o $RUNDIR/CR_18S_otu_table_w_tax_seqs.txt

# Filter out non 18S sequences
	# The V9 primers are 3-domain primers so you will get Bacteria and Archaea in your data
srun filter_taxa_from_otu_table.py -i $RUNDIR/CR_18S_otu_table_w_tax.biom \
	-o $RUNDIR/CR_18S_otu_table_w_tax_EUK.biom \
	-n Bacteria,Archaea
	
srun biom summarize-table -i $RUNDIR/CR_18S_otu_table_w_tax_EUK.biom \
	-o $RUNDIR/CR_18S_otu_table_w_tax_EUK_seqs.txt

# Remove host sequences
srun filter_taxa_from_otu_table.py -i $RUNDIR/CR_18S_otu_table_w_tax_EUK.biom \
	-o $RUNDIR/CR_18S_otu_table_w_tax_EUK_NOhost.biom \
	-n __Arthropoda
	
srun biom summarize-table -i $RUNDIR/CR_18S_otu_table_w_tax_EUK_NOhost.biom \
	-o $RUNDIR/CR_18S_otu_table_w_tax_EUK_NOhost_seqs.txt

# Filter low abundance OTUs (< 0.001%)
	# Conservative, (<0.005%) recommended for Illumina data
srun filter_otus_from_otu_table.py -i $RUNDIR/CR_18S_otu_table_w_tax_EUK_NOhost.biom \
	-o $RUNDIR/CR_18S_otu_table_w_tax_EUK_NOhost_af.biom \
	--min_count_fraction 0.00001

srun biom summarize-table -i $RUNDIR/CR_18S_otu_table_w_tax_EUK_NOhost_af.biom \
	-o $RUNDIR/CR_18S_otu_table_w_tax_EUK_NOhost_af_seqs.txt

# Filter out samples with less than 5,000 sequences and their pairs (list in the CR_18S_samples_to_keep.txt file)
srun filter_samples_from_otu_table.py -i $RUNDIR/CR_18S_otu_table_w_tax_EUK_NOhost_af.biom \
	-o $RUNDIR/CR_18S_otu_table_w_tax_EUK_NOhost_af_min5000.biom \
	--sample_id_fp $RUNDIR/CR_18S_samples_to_keep_reanalysis.txt

# Filter OTUs not in any samples from OTU table
srun filter_otus_from_otu_table.py -i $RUNDIR/CR_18S_otu_table_w_tax_EUK_NOhost_af_min5000.biom \
	-s 1 -o $RUNDIR/CR_18S_otu_table_w_tax_EUK_NOhost_af_min5000_filtered.biom

# Sort the OTU table 
sort_otu_table.py -i $RUNDIR/CR_18S_otu_table_w_tax_EUK_NOhost_af_min5000_filtered.biom \
	-o $RUNDIR/CR_18S_otu_table_w_tax_EUK_NOhost_af_min5000_filtered_sorted.biom
	
# Summarize the taxonomy
srun summarize_taxa.py -i $RUNDIR/CR_18S_otu_table_w_tax_EUK_NOhost_af_min5000_filtered_sorted.biom -o $RUNDIR -L 13


	
############### Alpha Diversity Analyses ##############	

module load qiime/intel/1.9.1 

srun single_rarefaction.py -i CR_18S_otu_table_w_tax_EUK_NOhost_af_min5000_filtered_sorted.biom \
	-o CR_18S_otu_table_w_tax_EUK_NOhost_af_min5000_filtered_sorted_rarefaction_5000.biom -d 5000

srun alpha_diversity.py -i CR_18S_otu_table_w_tax_EUK_NOhost_af_min5000_filtered_sorted_rarefaction_5000.biom \
	-m shannon -o CR_18S_otu_table_w_tax_EUK_af_min5000_filtered_sorted_alpha_5000.txt

# Comparison and statistical tests of Alpha diversity and all Beta diversity calculations were performed in R
	# See CR_18S_analysis_R.txt
	